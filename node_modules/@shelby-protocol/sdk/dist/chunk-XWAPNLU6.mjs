import {
  concatHashes,
  readInChunks,
  zeroPadBytes
} from "./chunk-4JZO2D7T.mjs";
import {
  DEFAULT_CHUNKSET_SIZE_BYTES
} from "./chunk-67F5YZ25.mjs";
import {
  DEFAULT_ERASURE_K,
  DEFAULT_ERASURE_M
} from "./chunk-ZPW742E7.mjs";

// src/core/commitments.ts
import { Hex } from "@aptos-labs/ts-sdk";
import { z } from "zod";
var ChunksetCommitmentSchema = z.object({
  // Chunkset root (vector commitment of child chunks)
  chunkset_root: z.string(),
  // the size is known statically from the current configuration
  chunk_commitments: z.array(z.string())
}).refine(
  (data) => {
    return data.chunk_commitments.length === DEFAULT_ERASURE_K + DEFAULT_ERASURE_M;
  },
  {
    message: `Chunkset must have exactly ${DEFAULT_ERASURE_K + DEFAULT_ERASURE_M} chunks (ERASURE_K + ERASURE_M = ${DEFAULT_ERASURE_K} + ${DEFAULT_ERASURE_M})`,
    path: ["chunk_commitments"]
  }
);
function expectedTotalChunksets(rawSize, chunksetSize = DEFAULT_CHUNKSET_SIZE_BYTES) {
  if (chunksetSize <= 0) {
    throw new Error("chunksetSize must be positive");
  }
  if (rawSize === 0) return 1;
  return Math.ceil(rawSize / chunksetSize);
}
var BlobCommitmentsSchema = z.object({
  schema_version: z.string(),
  raw_data_size: z.number(),
  // FIXME I am not sure about this being here, or if it should be somewhere else
  blob_merkle_root: z.string(),
  chunkset_commitments: z.array(ChunksetCommitmentSchema)
}).refine(
  (data) => {
    return expectedTotalChunksets(data.raw_data_size) === data.chunkset_commitments.length;
  },
  {
    message: "Total chunkset count mismatches with raw data size",
    // FIXME put more details in here
    path: ["chunkset_commitments"]
  }
);
async function generateMerkleRoot(leafHashes) {
  if (!leafHashes.length) {
    throw new Error(
      "An empty array cannot be used to construct a Merkle tree."
    );
  }
  const zeroArray = new Uint8Array(leafHashes[0].toUint8Array().length);
  const zeroBytes = Hex.fromHexInput(zeroArray);
  let currentLeaves = leafHashes;
  while (currentLeaves.length > 1) {
    if (currentLeaves.length % 2 !== 0) {
      currentLeaves.push(zeroBytes);
    }
    const nextLeaves = [];
    for (let i = 0; i < currentLeaves.length; i += 2) {
      nextLeaves.push(
        await concatHashes([
          currentLeaves[i].toUint8Array(),
          currentLeaves[i + 1].toUint8Array()
        ])
      );
    }
    currentLeaves = nextLeaves;
  }
  return currentLeaves[0];
}
async function generateChunksetCommitments(shouldPad, chunksetIdx, chunksetData, expectedChunksetSize, provider, onChunk) {
  const { erasure_n } = provider.config;
  const chunksetPayload = shouldPad ? zeroPadBytes(chunksetData, expectedChunksetSize) : validatePrePaddedChunkset(
    chunksetData,
    expectedChunksetSize,
    chunksetIdx
  );
  const { chunks } = provider.encode(chunksetPayload);
  if (chunks.length !== erasure_n) {
    throw new Error(
      `Erasure provider produced ${chunks.length} chunks, expected ${erasure_n}.`
    );
  }
  const chunkRoots = provider.getChunkMerkleRoots();
  let chunkIdx = 0;
  for (const chunkData of chunks) {
    if (onChunk !== void 0) {
      await onChunk(chunksetIdx, chunkIdx, chunkData);
    }
    chunkIdx += 1;
  }
  const a = await generateMerkleRoot(
    chunkRoots.map((a2) => Hex.fromHexInput(a2))
  );
  const entry = {
    chunkset_root: a.toString(),
    chunk_commitments: chunkRoots.map(
      (chunk) => Hex.fromHexInput(chunk).toString()
    )
  };
  return { h: a, entry };
}
async function generateCommitments(provider, fullData, onChunk, options) {
  const expectedChunksetSize = DEFAULT_CHUNKSET_SIZE_BYTES;
  const shouldPad = options?.pad ?? true;
  const chunksetCommitments = [];
  const chunksetCommitmentHashes = [];
  let rawDataSize = 0;
  const chunksetGen = readInChunks(fullData, expectedChunksetSize);
  for await (const [chunksetIdx, chunksetData] of chunksetGen) {
    rawDataSize += chunksetData.length;
    const { h, entry } = await generateChunksetCommitments(
      shouldPad,
      chunksetIdx,
      chunksetData,
      expectedChunksetSize,
      provider,
      onChunk
    );
    chunksetCommitments.push(entry);
    chunksetCommitmentHashes.push(h);
  }
  if (rawDataSize === 0) {
    const zeroChunkset = new Uint8Array(expectedChunksetSize);
    const { h, entry } = await generateChunksetCommitments(
      shouldPad,
      0,
      zeroChunkset,
      expectedChunksetSize,
      provider,
      onChunk
    );
    chunksetCommitments.push(entry);
    chunksetCommitmentHashes.push(h);
  }
  return {
    schema_version: "1.3",
    raw_data_size: rawDataSize,
    blob_merkle_root: (await generateMerkleRoot(chunksetCommitmentHashes)).toString(),
    chunkset_commitments: chunksetCommitments
  };
}
function validatePrePaddedChunkset(chunkset, expectedSize, chunksetIdx) {
  if (chunkset.byteLength !== expectedSize) {
    throw new Error(
      `Chunkset ${chunksetIdx} has size ${chunkset.byteLength} bytes but expected ${expectedSize} bytes. Enable padding or supply pre-padded data before calling generateCommitments.`
    );
  }
  return chunkset;
}

export {
  ChunksetCommitmentSchema,
  expectedTotalChunksets,
  BlobCommitmentsSchema,
  generateMerkleRoot,
  generateCommitments
};
